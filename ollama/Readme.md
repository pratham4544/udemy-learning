
# RAG Chatbot using Ollama for LLM & Embeddings

This project demonstrates how to create a Retrieval-Augmented Generation (RAG) chatbot using Ollama for LLM and embeddings.

## Overview

RAG combines the strengths of retrieval-based and generative models to provide more accurate and contextually relevant responses. This project leverages Ollama for implementing LLM and embeddings to enhance the chatbot's performance.

## Features

- **Retrieval-Augmented Generation**: Integrates retrieval mechanisms with generative responses.
- **Ollama Integration**: Uses Ollama for managing the LLM and embedding processes.
- **Improved Contextual Understanding**: Enhances the chatbot's ability to understand and generate contextually appropriate responses.

## Requirements

- Python 3.x
- Jupyter Notebook
- Ollama
- Other dependencies listed in `requirements.txt`

## Installation

1. Clone the repository:

   \`\`\`bash
   git clone https://github.com/yourusername/rag-chatbot.git
   cd rag-chatbot
   \`\`\`

2. Create a virtual environment and activate it:

   \`\`\`bash
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   \`\`\`

3. Install the required packages:

   \`\`\`bash
   pip install -r requirements.txt
   \`\`\`

## Usage

1. Launch Jupyter Notebook:

   \`\`\`bash
   jupyter notebook
   \`\`\`

2. Open the `ollama-final.ipynb` notebook and run the cells to set up and interact with the RAG chatbot.

## Project Structure

- \`ollama-final.ipynb\`: The main Jupyter notebook containing the implementation of the RAG chatbot.
- \`requirements.txt\`: List of dependencies required to run the project.
- \`README.md\`: Project documentation.

## Acknowledgements

This project uses Ollama for LLM and embeddings. For more information, visit [Ollama's official website](https://ollama.com).

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.